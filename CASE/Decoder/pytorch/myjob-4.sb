#!/bin/bash --login
 
#SBATCH --time=0:39:59            # limit of wall clock time - how long the job will run (same as -t)
#SBATCH --exclude=lac-143
#SBATCH --nodes=1                 # number of different nodes - could be an exact number or a range of nodes (same as -N)
#SBATCH --ntasks=1                  # number of tasks - how many tasks (nodes) that you require (same as -n)
#SBATCH --cpus-per-task=4           # number of CPUs (or cores) per task (same as -c)
#SBATCH --mem-per-cpu=8G            # memory required per allocated CPU (or core) - amount of memory (in bytes)
#SBATCH --job-name cl-lora      # you can give your job a name for easier identification (same as -J)
#SBATCH --gres=gpu:v100:1
########## Command Lines for Job Running ##########

module purge
module load GCC/6.4.0-2.28 OpenMPI  ### load necessary modules.
module load CUDA/10.0.130 cuDNN/7.5.0.56-CUDA-10.0.130
module load Python/3.6.4
# I can run my code on my conda env (pytorch) 
cd /mnt/home/renyidon/mobicom2021/code/pytorch   ### change to the directory where your code is located.
 
srun python main.py --dir_comment bit2 --server --normalization --train_iters 0 --network end2end --y_spectrogram 64 --scaling_factor 4 --coding_config 1 --load_iters 100000 --min_snr -50 --load pre_trained --data_dir lora_outdoor_trace #ndoor_cross_domain ### call your executable. (use srun instead of mpirun.)
 
scontrol show job $SLURM_JOB_ID     ### write job information to SLURM output file.
#js -j $SLURM_JOB_ID                 ### write resource usage to SLURM output file (powertools command).
